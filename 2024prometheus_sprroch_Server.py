# -*- coding: utf-8 -*-
"""2024prometheus_server_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jIWpvwEjkaMT2vD1VmCKZvgO-SQFNEEs
"""

! pip install python-dotenv
! pip install boto3
! pip install pillow
! pip install opencv-python matplotlib pillow tensorflow torch torchvision torchsummary

from zipfile import ZipFile
import os
import pandas as pd
import csv
import cv2

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

import numpy as np
from PIL import Image
import tensorflow as tf
from torch.utils.data import DataLoader, TensorDataset

import dotenv
import boto3

# PyTorch
import torch
from torch import nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
import torch.utils.data as data
from torch.utils.data import DataLoader, TensorDataset
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
from torch.optim import lr_scheduler
from torchvision import utils
from torchsummary import summary
from torchvision.models import resnet50
from io import BytesIO
from io import StringIO

# Access key ID 및 Secret access key
aws_access_key_id = 'AKIAQ3EGTFYWEDXCVAY5'
aws_secret_access_key = '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'

# S3 bucket 이름 및 리전
bucket_name = 'scalpyimages'
region_name = 'ap-northeast-2'

# S3 클라이언트 생성
s3_client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)

# S3 버킷 객체 생성
s3_resource = boto3.resource('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)
bucket = s3_resource.Bucket(bucket_name)

# S3 버킷 내의 객체 목록 확인
for obj in bucket.objects.all():
    print(obj.key)

def s3_load_image(img_key):
    try:
        # S3에서 이미지 객체 가져오기
        img_obj = bucket.Object(img_key)
        img_data = img_obj.get()['Body'].read()

        # BytesIO 객체를 사용하여 PIL Image로 변환
        img = Image.open(BytesIO(img_data))
        return img
    except Exception as e:
        print(f"Error loading image from S3: {e}")
        return None

# 이미지 키 (S3 버킷 내의 객체 경로)
img_key = 'images/scalpy.jpg'

# test_path = '/content/drive/MyDrive/2024prometheus/Hair Diseases - Final/input/user_image.jpg'

IMG_SIZE = 256

test_data = []

# 이미지 읽기
img = s3_load_image(img_key)
# 이미지 리사이즈
img = img.resize((IMG_SIZE, IMG_SIZE))
# PIL 이미지를 numpy 배열로 변환
img_np = np.array(img)
# 노이즈 제거
median_filtered_image = cv2.medianBlur(img_np, 3)
one_img = np.asarray(np.float32(median_filtered_image))
norm_img = one_img / 255.0

# 데이터에 추가
test_data.append(norm_img)

# 테스트 데이터를 numpy 배열로 변환
test_data = np.array(test_data)

# 테스트 데이터 확인
print("Test Data shape:", test_data.shape)

test_data = test_data.transpose((0, 3, 1, 2))

# 데이터 확인
print("Test Data shape:", test_data.shape)

# model_path = '/content/drive/MyDrive/2024prometheus/multi_best_model.pth'

def s3_load_model(model_key, bucket_name, aws_access_key_id, aws_secret_access_key, region_name):
    try:
        # S3 클라이언트 및 버킷 생성
        s3_client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)
        s3_resource = boto3.resource('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)
        bucket = s3_resource.Bucket(bucket_name)

        # S3에서 모델 객체 가져오기
        model_obj = bucket.Object(model_key)
        model_data = model_obj.get()['Body'].read()

        # BytesIO 객체를 사용하여 모델 로드
        loaded_model = torch.load(BytesIO(model_data), map_location='cpu')
        return loaded_model
    except Exception as e:
        print(f"Error loading model from S3: {e}")
        return None

model_key = 'multi_best_model.pth'

# 모델 정의
class MultiBinaryClassificationModel(nn.Module):
    def __init__(self, num_classes=10) -> None:
        super(MultiBinaryClassificationModel, self).__init__()
        self.resnet = resnet50(pretrained=True)
        # 다중 이진 분류를 위한 출력 레이어 - 뉴런 수를 클래스의 개수에 맞게 조정하고 Sigmoid 적용
        self.classifier = nn.Linear(1000, num_classes)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.resnet(x)
        x = self.classifier(x)
        x = self.sigmoid(x)  # Sigmoid 적용

        return x

# 현재 사용 가능한 디바이스 확인
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 테스트 데이터를 모델 입력 형식에 맞게 변환
test_tensor = torch.from_numpy(test_data).float().to(device)

# S3에서 모델 로드
loaded_model = s3_load_model(model_key, bucket_name, aws_access_key_id, aws_secret_access_key, region_name)
loaded_model.eval()

# 로드된 모델과 입력 데이터를 동일한 디바이스로 이동
loaded_model = loaded_model.to(device)

# 모델에 입력 데이터 전달하여 예측 수행
with torch.no_grad():
    outputs = loaded_model(test_tensor)

# 예측 결과를 CPU로 이동하고 NumPy 배열로 변환
predictions = outputs.cpu().numpy()

# S3에 저장할 디렉토리 및 파일 이름 설정
s3_output_key = 'predictions/predictions.csv'

# 결과를 DataFrame으로 변환
df = pd.DataFrame(predictions, columns=['Alopecia Areata', 'Contact Dermatitis', 'Folliculitis', 'Head Lice',
                                        'Lichen Planus', 'Male Pattern Baldness', 'Psoriasis',
                                        'Seborrheic Dermatitis', 'Telogen Effluvium', 'Tinea Capitis'])

# 이미지 파일 경로 추가
df.insert(0, 'Image_Path', [img_key])  # img_key 변수를 사용하여 이미지 파일 경로 추가

# DataFrame을 CSV 형식으로 변환
csv_content = df.to_csv(index=False)

# CSV 파일을 S3에 업로드
s3_client.put_object(Body=csv_content, Bucket=bucket_name, Key=s3_output_key)

# 저장된 CSV 파일의 S3 경로
s3_output_path = f's3://{bucket_name}/{s3_output_key}'
print(f"예측 결과가 다음 경로에 저장되었습니다: {s3_output_path}")

# # 결과를 CSV 파일로 저장
# csv_file_path = '/content/drive/MyDrive/2024prometheus/Hair Diseases - Final/predictions.csv'

# # 결과를 DataFrame으로 변환
# df = pd.DataFrame(predictions, columns=['Alopecia Areata', 'Contact Dermatitis', 'Folliculitis', 'Head Lice',
#                                         'Lichen Planus', 'Male Pattern Baldness', 'Psoriasis',
#                                         'Seborrheic Dermatitis', 'Telogen Effluvium', 'Tinea Capitis'])

# # 이미지 파일 경로 추가
# df.insert(0, 'Image_Path', [test_path])

# # DataFrame을 CSV 파일로 저장
# df.to_csv(csv_file_path, index=False)

# # 저장된 CSV 파일 확인
# print(f"예측 결과가 다음 경로에 저장되었습니다: {csv_file_path}")